import pandas as pd
import torch
import re
import kss
import os
from tqdm import tqdm
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration

# í•˜ë“œì½”ë”©ëœ ê²½ë¡œ ë° ì„¤ì •ê°’
INPUT_CSV_PATH = r"C:\Users\ê°±ë³´\Desktop\ê°±ë³´\2025\1-2025\ì¢…í”„\ëª¨ë¸ í•™ìŠµ\dataset\ê¸°ì‚¬ ë°ì´í„°\complete_test_stratified_utf8.csv"
OUTPUT_CSV_BASE = r"C:\Users\ê°±ë³´\Desktop\ê°±ë³´\2025\1-2025\ì¢…í”„\ëª¨ë¸ í•™ìŠµ\dataset\ë¬¸ì¥ë¶„í•´ê²°ê³¼"  # í™•ì¥ì ì œì™¸ (ë²ˆí˜¸ ë¶™ì„)
SUMMARY_RATIO = 0.3  # ìš”ì•½ ë¹„ìœ¨: ì›ë³¸ì˜ 30%
SAVE_INTERVAL = 500  # ì¤‘ê°„ ê²°ê³¼ ì €ì¥ ê°„ê²© (ê¸°ì‚¬ ìˆ˜)

class KoBARTSummarizer:
    def __init__(self):
        """KoBART ìš”ì•½ ëª¨ë¸ ì´ˆê¸°í™”"""
        print("ğŸ”„ KoBART ìš”ì•½ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-summarization')
        self.model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-summarization')
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        print(f"KoBART ëª¨ë¸ ì¥ì¹˜: {self.device}")
        self.model.eval()

    def summarize(self, text, ratio=SUMMARY_RATIO):
        """
        ê¸°ì‚¬ ìš”ì•½ - ì›ë³¸ ê¸¸ì´ì˜ ì¼ì • ë¹„ìœ¨ë¡œ ìš”ì•½
        ratio: ì›ë³¸ ëŒ€ë¹„ ìš”ì•½ ë¹„ìœ¨ (0.3 = 30%)
        """
        if not text or len(text) < 50:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ìš”ì•½í•˜ì§€ ì•ŠìŒ
            return text
            
        try:
            # ë¶ˆí•„ìš”í•œ ì •ë³´ ì œê±° (ì´ë©”ì¼, ê¸°ì ì •ë³´ ë“±)
            text = clean_unnecessary_info(text)
            
            # ì›ë³¸ ë¬¸ì¥ ìˆ˜ ê³„ì‚°
            original_sentences = split_sentences(text)
            original_sentence_count = len(original_sentences)
            
            # íƒ€ê²Ÿ ë¬¸ì¥ ìˆ˜ ê³„ì‚° (ìµœì†Œ 3ë¬¸ì¥, ìµœëŒ€ ì›ë³¸ì˜ ratio ë¹„ìœ¨)
            target_sentences = max(3, int(original_sentence_count * ratio))
            
            # ê¸¸ì´ê°€ ê¸´ ê²½ìš° ì ì ˆíˆ ìë¥´ê¸° (KoBART ì…ë ¥ ì œí•œ)
            if len(text) > 1024:
                text = text[:1024]
                
            inputs = self.tokenizer.encode(text, return_tensors="pt").to(self.device)
            
            # ìš”ì•½ ìƒì„±
            with torch.no_grad():
                # ë¬¸ì¥ ìˆ˜ì— ë”°ë¼ max_lengthì™€ min_length ì¡°ì •
                max_token_length = min(512, int(len(inputs[0]) * ratio * 1.5))
                min_token_length = max(50, int(len(inputs[0]) * ratio * 0.8))
                
                summary_ids = self.model.generate(
                    inputs,
                    max_length=max_token_length,
                    min_length=min_token_length,
                    length_penalty=1.0,  # ìš”ì•½ ê¸¸ì´ ì¡°ì ˆ
                    num_beams=4,
                    early_stopping=True
                )
                
            # ë””ì½”ë”©
            summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)
            
            # ë°˜ë³µ íŒ¨í„´ ë° ë¶ˆí•„ìš”í•œ ì •ë³´ ì œê±°
            summary = clean_repetitive_patterns(summary)
            summary = clean_unnecessary_info(summary)
            
            # ì¤‘ë³µ ë¬¸ì¥ ì œê±° ë° ì •ì œ
            summary = self.clean_summary(summary)
            
            # ì›í•˜ëŠ” ë¬¸ì¥ ìˆ˜ì— ë§ê²Œ ì¡°ì •
            summary_sentences = split_sentences(summary)
            
            # ìš”ì•½ëœ ë¬¸ì¥ì´ íƒ€ê²Ÿë³´ë‹¤ ì ìœ¼ë©´ ì›ë³¸ì—ì„œ ì¤‘ìš” ë¬¸ì¥ ì¶”ê°€
            if len(summary_sentences) < target_sentences:
                # ì›ë³¸ì—ì„œ ì²« ë¬¸ì¥ê³¼ ë§ˆì§€ë§‰ ë¬¸ì¥ (ë³´í†µ ì¤‘ìš”í•œ ì •ë³´ í¬í•¨)
                additional_sentences = []
                if original_sentence_count > 0:
                    # ì²« ë¬¸ì¥ ì¶”ê°€ (ìš”ì•½ì— ì—†ì„ ê²½ìš°)
                    if not any(sentence_similarity(original_sentences[0], s) > 0.7 for s in summary_sentences):
                        first_sent = clean_repetitive_patterns(original_sentences[0])
                        first_sent = clean_unnecessary_info(first_sent)
                        if is_valid_sentence(first_sent):
                            additional_sentences.append(first_sent)
                    
                    # ë§ˆì§€ë§‰ ë¬¸ì¥ ì¶”ê°€ (ìš”ì•½ì— ì—†ê³  ì´ë©”ì¼ì´ë‚˜ ê¸°ì ì •ë³´ê°€ ì•„ë‹Œ ê²½ìš°)
                    last_sent = original_sentences[-1]
                    if not any(sentence_similarity(last_sent, s) > 0.7 for s in summary_sentences) and \
                       not contains_email(last_sent) and not contains_reporter_info(last_sent):
                        last_sent = clean_repetitive_patterns(last_sent)
                        last_sent = clean_unnecessary_info(last_sent)
                        if is_valid_sentence(last_sent):
                            additional_sentences.append(last_sent)
                
                # ë‚¨ì€ ë¶€ì¡±í•œ ë¬¸ì¥ ìˆ˜ë¥¼ ì›ë³¸ì—ì„œ ì¶”ê°€
                remaining_needed = target_sentences - len(summary_sentences) - len(additional_sentences)
                if remaining_needed > 0:
                    # ì¤‘ê°„ ë¬¸ì¥ë“¤ì—ì„œ ì„ íƒ (ê¸¸ì´ê°€ ê¸´ ë¬¸ì¥ ìš°ì„ )
                    middle_sentences = original_sentences[1:-1] if len(original_sentences) > 2 else []
                    # ìœ íš¨í•œ ë¬¸ì¥ë§Œ ì„ íƒ
                    valid_middle_sentences = []
                    for s in middle_sentences:
                        if not any(sentence_similarity(s, existing) > 0.7 for existing in summary_sentences + additional_sentences) and \
                           not contains_email(s) and not contains_reporter_info(s):
                            s_clean = clean_repetitive_patterns(s)
                            s_clean = clean_unnecessary_info(s_clean)
                            if is_valid_sentence(s_clean):
                                valid_middle_sentences.append((s_clean, len(s_clean)))
                    
                    # ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ê³  í•„ìš”í•œ ë§Œí¼ ì¶”ê°€
                    valid_middle_sentences.sort(key=lambda x: x[1], reverse=True)
                    additional_sentences.extend([s[0] for s in valid_middle_sentences[:remaining_needed]])
                
                # ìš”ì•½ + ì¶”ê°€ ë¬¸ì¥ì„ í•©ì³ì„œ ìµœì¢… ìš”ì•½ ìƒì„±
                final_sentences = summary_sentences + additional_sentences
                
                # ì›ë³¸ ìˆœì„œëŒ€ë¡œ ì¬ì •ë ¬ (ì›ë³¸ ë¬¸ì¥ì˜ ìˆœì„œë¥¼ ë³´ì¡´)
                def get_original_position(sentence):
                    for i, orig in enumerate(original_sentences):
                        if sentence_similarity(sentence, orig) > 0.7:
                            return i
                    return 999  # ì¼ì¹˜í•˜ëŠ” ë¬¸ì¥ì´ ì—†ìœ¼ë©´ ë§¨ ë’¤ë¡œ
                
                final_sentences.sort(key=get_original_position)
                
                # ìµœì¢… ë¬¸ì¥ë“¤ ì •ì œ
                final_sentences = [clean_unnecessary_info(clean_repetitive_patterns(s)) for s in final_sentences]
                summary = ' '.join(final_sentences)
            
            return summary
        except Exception as e:
            print(f"ìš”ì•½ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ì •ë³´ ì œê±° í›„ ë°˜í™˜
            return clean_unnecessary_info(text)

    def clean_summary(self, summary):
        """ìš”ì•½ë¬¸ì—ì„œ ì¤‘ë³µëœ ë‚´ìš© ì œê±° ë° ì •ì œ"""
        # ì¤‘ë³µëœ ë¬¸ì¥ íŒ¨í„´ ì œê±°
        summary = re.sub(r'(.{20,}?)\1+', r'\1', summary)
        
        # ë¬¸ì¥ ë¶„ë¦¬
        try:
            sentences = kss.split_sentences(summary)
        except:
            sentences = re.split(r'(?<=[.!?])\s+', summary)
        
        # ì¤‘ë³µ ë¬¸ì¥ ì œê±°
        unique_sentences = []
        seen = set()
        
        for s in sentences:
            s_clean = s.strip()
            # í•µì‹¬ ë‚´ìš©ë§Œ ë¹„êµ (ì¡°ì‚¬, ì–´ë¯¸ ì œê±°)
            core_content = re.sub(r'[ì€ëŠ”ì´ê°€ì„ë¥¼ì—ì„œì˜ë¡œ]', '', s_clean)
            if len(s_clean) > 10 and core_content not in seen and is_valid_sentence(s_clean):
                seen.add(core_content)
                unique_sentences.append(s_clean)
        
        return ' '.join(unique_sentences)

def sentence_similarity(s1, s2):
    """ë‘ ë¬¸ì¥ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ êµ¬í˜„)"""
    # ì¡°ì‚¬, ì–´ë¯¸ ì œê±°í•œ í•µì‹¬ ë‚´ìš©ìœ¼ë¡œ ë¹„êµ
    s1_core = re.sub(r'[ì€ëŠ”ì´ê°€ì„ë¥¼ì—ì„œì˜ë¡œ]', '', s1)
    s2_core = re.sub(r'[ì€ëŠ”ì´ê°€ì„ë¥¼ì—ì„œì˜ë¡œ]', '', s2)
    
    # ë¬¸ìì—´ì„ ì§‘í•©ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ìì¹´ë“œ ìœ ì‚¬ë„ ê³„ì‚°
    if not s1_core or not s2_core:
        return 0
    
    s1_chars = set(s1_core)
    s2_chars = set(s2_core)
    
    intersection = len(s1_chars.intersection(s2_chars))
    union = len(s1_chars.union(s2_chars))
    
    return intersection / union if union > 0 else 0

def clean_repetitive_patterns(text):
    """ë°˜ë³µë˜ëŠ” íŒ¨í„´ ì œê±°"""
    # 1. ë™ì¼ ë¬¸ì 3íšŒ ì´ìƒ ë°˜ë³µ íŒ¨í„´ ì •ì œ (ì˜ˆ: "í‚¹í‚¹í‚¹í‚¹" -> "í‚¹")
    text = re.sub(r'([ê°€-í£])\1{2,}', r'\1', text)
    
    # 2. ë™ì¼ ë‹¨ì–´ ì—°ì† ë°˜ë³µ íŒ¨í„´ ì •ì œ (ì˜ˆ: "ì¢‹ì•„ ì¢‹ì•„ ì¢‹ì•„" -> "ì¢‹ì•„")
    word_repeat_pattern = re.compile(r'(([ê°€-í£]+) \2(?: \2)+)')
    for match in word_repeat_pattern.finditer(text):
        repeated_phrase = match.group(1)
        single_word = match.group(2)
        text = text.replace(repeated_phrase, single_word)
    
    # 3. 2ìŒì ˆ ì´ìƒ ë‹¨ì–´ ë°˜ë³µ íŒ¨í„´ ì œê±° (ì˜ˆ: "ì•ˆë…•í•˜ì„¸ìš”ì•ˆë…•í•˜ì„¸ìš”" -> "ì•ˆë…•í•˜ì„¸ìš”")
    for word_len in range(6, 1, -1):  # 6ìŒì ˆë¶€í„° 2ìŒì ˆê¹Œì§€ ê²€ì‚¬
        pattern = re.compile(f'([ê°€-í£]{{{word_len}}})\\1+')
        for match in pattern.finditer(text):
            repeated_word = match.group(0)
            single_word = match.group(1)
            text = text.replace(repeated_word, single_word)
    
    return text

def contains_email(text):
    """ì´ë©”ì¼ ì£¼ì†Œ í¬í•¨ ì—¬ë¶€ í™•ì¸"""
    email_pattern = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}')
    return bool(email_pattern.search(text))

def contains_reporter_info(text):
    """ê¸°ì ì •ë³´ í¬í•¨ ì—¬ë¶€ í™•ì¸"""
    reporter_patterns = [
        r'\sê¸°ì\s*$',
        r'\[.*?ê¸°ì\]',
        r'\(.*?ê¸°ì\)',
        r'[ê°€-í£]+\s*ê¸°ì\s*=',
        r'[ê°€-í£]+\s*ê¸°ì\s*[a-zA-Z0-9._%+-]+@',
        r'[â– â—†â—â–¶â–·â–¶ï¸]'  # ê¸°ì‚¬ ëì— ë§ì´ ì‚¬ìš©ë˜ëŠ” ê¸°í˜¸
    ]
    
    for pattern in reporter_patterns:
        if re.search(pattern, text):
            return True
    
    return False

def clean_unnecessary_info(text):
    """ë¶ˆí•„ìš”í•œ ì •ë³´ ì œê±° (ì´ë©”ì¼, ê¸°ì ì •ë³´ ë“±)"""
    # 1. ì´ë©”ì¼ ì£¼ì†Œ ì œê±°
    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '', text)
    
    # 2. ê¸°ì ì •ë³´ê°€ í¬í•¨ëœ ë¶€ë¶„ ì œê±°
    reporter_patterns = [
        r'\s*\[[ê°€-í£\s]+ê¸°ì\]\s*',
        r'\s*\([ê°€-í£\s]+ê¸°ì[=]?[ê°€-í£\s]*\)\s*',
        r'\s*[ê°€-í£]+\s*ê¸°ì\s*=\s*[ê°€-í£]+\s*',
        r'\s*[ê°€-í£]+\s*ê¸°ì$'
    ]
    
    for pattern in reporter_patterns:
        text = re.sub(pattern, ' ', text)
    
    # 3. ì–¸ë¡ ì‚¬ í‘œì‹œ ì œê±° (ì˜ˆ: [ì¤‘ì•™ì¼ë³´], [í•œê²¨ë ˆ] ë“±)
    text = re.sub(r'\s*\[[ê°€-í£A-Za-z\s]+\]\s*', ' ', text)
    
    # 4. ê¸°ì‚¬ ë‚ ì§œ ì •ë³´ ì œê±°
    text = re.sub(r'\s*\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼\s*', ' ', text)
    
    # 5. íŠ¹ìˆ˜ ê¸°í˜¸ë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ ì œê±° (ë§ì€ ê¸°ì‚¬ì—ì„œ ë ë¶€ë¶„ í‘œì‹œë¡œ ì‚¬ìš©)
    text = re.sub(r'[â– â—†â—â–¶â–·â–¶ï¸].*$', '', text)
    
    # 6. ì—°ì†ëœ ê³µë°± ì œê±° ë° ì •ë¦¬
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def is_valid_sentence(text):
    """ìœ íš¨í•œ ë¬¸ì¥ì¸ì§€ í™•ì¸"""
    # 10ì ë¯¸ë§Œ ì§§ì€ ë¬¸ì¥ í•„í„°ë§
    if len(text) < 10:
        return False
    
    # ì´ë©”ì¼ì´ë‚˜ ê¸°ì ì •ë³´ê°€ ìˆëŠ” ë¬¸ì¥ ì œê±°
    if contains_email(text) or contains_reporter_info(text):
        return False
    
    # íŠ¹ìˆ˜ ê¸°í˜¸ë¡œë§Œ êµ¬ì„±ëœ ë¬¸ì¥ ì œê±°
    if re.match(r'^[^\wê°€-í£]+$', text):
        return False
    
    return True

def split_sentences(text):
    """ë¬¸ì¥ ë¶„ë¦¬ í•¨ìˆ˜ (í–¥ìƒëœ ë²„ì „)"""
    # ì¤‘ë³µëœ ë¬¸ì¥ íŒ¨í„´ ì œê±°
    text = clean_repetitive_patterns(text)
    
    # ì´ë©”ì¼ ë° ê¸°ì ì •ë³´ ì œê±°
    text = clean_unnecessary_info(text)
    
    try:
        # KSSë¡œ ë¬¸ì¥ ë¶„ë¦¬ ì‹œë„
        sentences = kss.split_sentences(text)
    except Exception as e:
        # print(f"ğŸ” KSS ë¬¸ì¥ ë¶„ë¦¬ ì˜¤ë¥˜, ê¸°ë³¸ ë¶„ë¦¬ ì‚¬ìš©: {e}")
        # ê¸°ë³¸ êµ¬ë¶„ì ì‚¬ìš©
        sentences = re.split(r'(?<=[.!?])\s+', text)
    
    # ì¶”ê°€ì ì¸ ë¬¸ì¥ ë¶„ë¦¬ (ê¸´ ë¬¸ì¥ì´ë‚˜ ë³µí•© ë¬¸ì¥ ì²˜ë¦¬)
    expanded_sentences = []
    for sentence in sentences:
        # ë„ˆë¬´ ê¸´ ë¬¸ì¥ì¸ ê²½ìš° ì¶”ê°€ ë¶„ë¦¬
        if len(sentence) > 100:
            # ì‰¼í‘œë‚˜ ì„¸ë¯¸ì½œë¡  ë“±ìœ¼ë¡œ ì¶”ê°€ ë¶„ë¦¬
            sub_sentences = re.split(r'(?<=[,;])\s+', sentence)
            # ì¶©ë¶„íˆ ê¸´ ë¶€ë¶„ë§Œ ë³„ë„ ë¬¸ì¥ìœ¼ë¡œ ì²˜ë¦¬
            for sub in sub_sentences:
                if len(sub) >= 20:  # 20ì ì´ìƒë§Œ ë³„ë„ ë¬¸ì¥ìœ¼ë¡œ
                    expanded_sentences.append(sub)
                else:
                    # ì§§ì€ ë¶€ë¶„ì€ ì´ì „ ë¬¸ì¥ì— ë¶™ì´ê±°ë‚˜ ìƒˆ ë¬¸ì¥ìœ¼ë¡œ ì‹œì‘
                    if expanded_sentences:
                        expanded_sentences[-1] = expanded_sentences[-1] + ' ' + sub
                    else:
                        expanded_sentences.append(sub)
        else:
            expanded_sentences.append(sentence)
    
    # ì¤‘ë³µ ë° ì§§ì€ ë¬¸ì¥ ì œê±°, ìœ íš¨ì„± ê²€ì‚¬
    unique_sentences = []
    seen = set()
    
    for s in expanded_sentences:
        s_clean = s.strip()
        
        # ìœ íš¨í•˜ì§€ ì•Šì€ ë¬¸ì¥ ë¬´ì‹œ
        if not is_valid_sentence(s_clean):
            continue
            
        # í•µì‹¬ ë‚´ìš©ë§Œ ë¹„êµ (ì¡°ì‚¬, ì–´ë¯¸ ì œê±°)
        core_content = re.sub(r'[ì€ëŠ”ì´ê°€ì„ë¥¼ì—ì„œì˜ë¡œ]', '', s_clean)
        
        if len(s_clean) > 10 and core_content not in seen:
            seen.add(core_content)
            unique_sentences.append(s_clean)
    
    return unique_sentences

def further_split_sentence(sentence):
    """ë³µí•© ë¬¸ì¥ì„ ë” ì‘ì€ ë¬¸ì¥ìœ¼ë¡œ ì¶”ê°€ ë¶„ë¦¬"""
    result = []
    
    # ì´ë¯¸ ì¶©ë¶„íˆ ì§§ì€ ë¬¸ì¥ì€ ë¶„ë¦¬í•˜ì§€ ì•ŠìŒ
    if len(sentence) < 50:
        return [sentence]
        
    # ì ‘ì†ì‚¬ë‚˜ íŠ¹ì • íŒ¨í„´ìœ¼ë¡œ ë¶„ë¦¬
    connectors = [
        'ê·¸ë¦¬ê³ ', 'ë˜í•œ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ê·¸ëŸ°ë°', 'í•˜ë©°', 'ë”°ë¼ì„œ', 
        'ì´ì—', 'ì´ë•Œ', 'ë˜', 'ê·¸ë˜ì„œ', 'ì´ì–´', 'ë°˜ë©´', 'í•œí¸'
    ]
    
    # ì ‘ì†ì‚¬ ìœ„ì¹˜ ì°¾ê¸°
    positions = []
    for connector in connectors:
        pattern = f'\\s{connector}\\s'
        for match in re.finditer(pattern, sentence):
            positions.append((match.start(), match.end(), connector))
    
    # ìœ„ì¹˜ ì •ë ¬
    positions.sort(key=lambda x: x[0])
    
    # ë¶„í• ì ì´ ì—†ìœ¼ë©´ ì›ë˜ ë¬¸ì¥ ë°˜í™˜
    if not positions:
        return [sentence]
    
    # ë¬¸ì¥ ë¶„í• 
    prev_end = 0
    for start, end, _ in positions:
        # ë¶„í• ì  ì•ë¶€ë¶„ì´ ì¶©ë¶„íˆ ê¸´ ê²½ìš°ë§Œ ë³„ë„ ë¬¸ì¥ìœ¼ë¡œ
        if start - prev_end >= 15:  # ìµœì†Œ 15ì ì´ìƒ
            sub_sentence = sentence[prev_end:start].strip()
            if is_valid_sentence(sub_sentence):
                result.append(sub_sentence)
        prev_end = end
    
    # ë§ˆì§€ë§‰ ë¶€ë¶„ ì¶”ê°€
    if len(sentence) - prev_end >= 15:
        sub_sentence = sentence[prev_end:].strip()
        if is_valid_sentence(sub_sentence):
            result.append(sub_sentence)
    
    # ë¶„ë¦¬ëœ ê²ƒì´ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
    if not result:
        return [sentence]
        
    return result

def save_results(sentences, file_number):
    """ê²°ê³¼ ì €ì¥ í•¨ìˆ˜ - ìˆœì°¨ì  íŒŒì¼ ë²ˆí˜¸ ì‚¬ìš©"""
    # íŒŒì¼ ê²½ë¡œ ìƒì„± (ë¬¸ì¥ë¶„í•´ê²°ê³¼1.csv, ë¬¸ì¥ë¶„í•´ê²°ê³¼2.csv, ...)
    output_path = f"{OUTPUT_CSV_BASE}{file_number}.csv"
    
    # ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì €ì¥
    df = pd.DataFrame(sentences)
    df.to_csv(output_path, index=False)
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path} (ë¬¸ì¥ ìˆ˜: {len(sentences)})")
    
    return output_path

def main():
    """ë©”ì¸ í•¨ìˆ˜ - í•˜ë“œì½”ë”©ëœ ê²½ë¡œ ë° ì„¤ì • ì‚¬ìš©"""
    # ìš”ì•½ê¸° ì´ˆê¸°í™”
    summarizer = KoBARTSummarizer()
    
    print(f"ğŸ“‘ ê¸°ì‚¬ CSV ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘... ({INPUT_CSV_PATH})")
    df = pd.read_csv(INPUT_CSV_PATH)
    print(f"ì´ {len(df)}ê°œ ê¸°ì‚¬ ë¡œë“œë¨")
    
    # label1 ì—´ ì¡´ì¬ í™•ì¸
    has_label1 = 'label1' in df.columns
    if has_label1:
        print("âœ… label1 ì—´ ë°œê²¬: ê²°ê³¼ì— í¬í•¨ë©ë‹ˆë‹¤.")
    else:
        print("âš ï¸ label1 ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²°ê³¼ì— í¬í•¨ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    all_sentences = []
    processed_articles = 0
    file_number = 1  # ê²°ê³¼ íŒŒì¼ ìˆœì°¨ ë²ˆí˜¸
    
    # ê° ê¸°ì‚¬ ì²˜ë¦¬
    for i, row in tqdm(df.iterrows(), total=len(df), desc="ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘"):
        article_id = i + 1
        content = str(row.get("content", ""))
        
        # label1 ê°’ ê°€ì ¸ì˜¤ê¸° (ìˆëŠ” ê²½ìš°)
        label1_value = row.get("label1", None) if has_label1 else None
        
        if pd.isna(content) or content.strip() == "":
            continue
        
        # ì›ë³¸ ë¬¸ì¥ ìˆ˜ í™•ì¸
        original_sentences = split_sentences(content)
        original_count = len(original_sentences)
        
        # ê¸°ì‚¬ ìš”ì•½ (ì›ë³¸ì˜ ratio ë¹„ìœ¨ë¡œ)
        print(f"\nğŸ“° ê¸°ì‚¬ #{article_id} ìš”ì•½ ì¤‘... (ì›ë³¸: {original_count}ë¬¸ì¥)")
        summarized_text = summarizer.summarize(content, ratio=SUMMARY_RATIO)
        
        # ìš”ì•½ëœ ë¬¸ì¥ ì¶”ì¶œ ë° ì¶”ê°€ ë¶„ë¦¬
        sentences = []
        for sentence in split_sentences(summarized_text):
            # ê° ë¬¸ì¥ì— ëŒ€í•´ ì¶”ê°€ ë¶„ë¦¬ ìˆ˜í–‰ (ë³µí•© ë¬¸ì¥ ë¶„ë¦¬)
            sub_sentences = further_split_sentence(sentence)
            sentences.extend(sub_sentences)
        
        print(f"ğŸ“ ìš”ì•½ ê²°ê³¼: {len(sentences)}ë¬¸ì¥ ({len(sentences)/max(1, original_count)*100:.1f}%)")
        
        # ê° ë¬¸ì¥ì„ ê°œë³„ í–‰ìœ¼ë¡œ ì €ì¥ (label1 í¬í•¨)
        for sentence in sentences:
            # ìµœì¢… ìœ íš¨ì„± ê²€ì‚¬
            if not is_valid_sentence(sentence):
                continue
                
            sentence_data = {
                "article_id": article_id,
                "sentence": sentence
            }
            
            # label1ì´ ìˆìœ¼ë©´ ì¶”ê°€
            if has_label1 and label1_value is not None:
                sentence_data["label1"] = label1_value
                
            all_sentences.append(sentence_data)
        
        processed_articles += 1
        
        # ì§€ì •ëœ ê°„ê²©(SAVE_INTERVAL)ë§ˆë‹¤ ì¤‘ê°„ ê²°ê³¼ ì €ì¥í•˜ê³  ìƒˆ íŒŒì¼ ì‹œì‘
        if processed_articles % SAVE_INTERVAL == 0:
            save_results(all_sentences, file_number)
            file_number += 1  # ë‹¤ìŒ íŒŒì¼ ë²ˆí˜¸ë¡œ ì¦ê°€
            all_sentences = []  # ìƒˆ íŒŒì¼ì„ ìœ„í•´ ì´ˆê¸°í™”
    
    # ë§ˆì§€ë§‰ ë°°ì¹˜ ê²°ê³¼ ì €ì¥ (ë‚¨ì€ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
    if all_sentences:
        save_results(all_sentences, file_number)
    
    print(f"âœ… ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ! ì´ {processed_articles}ê°œ ê¸°ì‚¬ë¥¼ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
